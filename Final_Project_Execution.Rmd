---
title: "IS 607 Final Project"
author: "The Karens"
date: "Fall 2015"
output: 
  html_document: 
    toc: true
---

# Preliminary Project Overview

## Project Title:

Satire and Data Science : An Exploration into one of the Current Final Frontiers


## Project Participants: 

Joy Payton and Karen Weigandt

## Motivation and Inspiration:

This project is inspired by the Hutzler 571 Banana Slicer, and other products that really do exist, as hard as it is to believe (like Bic Cristal For Her pens and theft deterrent moldy sandwich bags) .  Joy Payton is the originator of the idea for the analysis, and Karen Weigandt is a collector of kitchen gadgets of the incredibly useless variety.

This project is actually not only fun and entertaining, it is based on real social need.  Satire is used as a tool to inspire change through humor and wit.  It allows us to look at situations with a critical eye, and hopefully grow with the introspection that naturally follows.  For many in the Asperger's and autism community, this understanding does not come naturally, though the capability and intelligence lies within.  In learning how to break it down into something that can be analyzed by a computer, we need to dissect the elements down into quantifiable differences, which can become teachable moments.  This is a process that can be useful in a variety of situations, as our society and interactions become more complex as time goes on.

## Project Goals:

In this project, we plan to examine satire in Amazon reviews and attempt to teach a computer to recognize what many humans cannot.  This way, we may be able to come up with some hermeneutic "rules of thumb" to help people figure out when a written text is meant to make fun of someone or something.  The craft of satire / poking fun at a situation, product, or person is intrinsically satisfying to satirists, sometimes especially if those being made fun of are unable to figure it out on their own.

The desired outcome is that we gain knowledge of data science concepts, both within and beyond those encountered in the IS607 curriculum. We hope to execute a data science workflow that includes obtaining data through webscraping and/or using tools like APIs and existing public databases, scrubbing, exploring and modeling the data using a variety of packages available in R, and finally interpreting the data using statistical and visualization tools to present our findings and conclusions.

## Data Science Workflow

For our workflow, we have chosen to use the OSEMN model:  We will 

- Obtain
- Scrub
- Explore
- Model, and
_ iNterpret 

Amazon product reviews with an eye toward developing a satire detection model.

## A Few Caveats

After triggering the Amazon "robot check" several times (and being confused as to why data extraction was failing!), we have placed Sys.sleep() commands in a number of places to keep this from happening.  This means that the script takes a surprisingly long time to run.  This is a known issue and a condition of data gathering.

Another challenge was the offensive word list used, which was comprehensive to the point of being overly sensitive. 

# Obtaining and Scrubbing Data

## Introduction to the Data

The URL for the first page of the Hutzler Banana Slicer can be found here: <http://www.amazon.com/Hutzler-571-Banana-Slicer/product-reviews/B0047E0EII/?sortBy=byRankDescending&pageNumber=1>.  Additional pages can be found by changing the last GET parameter to pageNumber=2, 3, etc.

By using the element inspector of Chrome, we can peek into the html structure and discover that each review is structured in this way:
```
div with class "reviews" holds all reviews
  div with class "review" holds a single review
    div with class "helpful-votes-count" has text describing how helpful the review has been to other users
    span with class "a-icon-alt" has text describing the number of stars
    \<a\> with class "review-title" has title of the review
    \<a\> with class "author" has the username of the reviewer
    span with class "review-date" has text containing the date of the review
    span with class "review-text" contains the review text
```    
## Tool Preparation

First, we need to install and load rvest, dplyr, and stringi.  We'll also load a few packages even though I'm not using them just yet.  They'll come in handy later!  Note that you may have to install packages that are not present in your own R environment.
```{r}
library(rvest)
library(stringi)
library(dplyr)
library(stringr)
```
## Getting the Amazon Reviews

In order to create a data frame with review text and related metadata, we will create a function that will accomplish several tasks:  
- It will download the first n pages of reviews (default is n=20) and store the html in the file system of the local computer.
- It will then use rvest to extract only the review from the html files saved to the file system
- Finally, it will clean the data and return a data frame.

```{r}
# BaseURL should be of the form 
getReviews <-function(base_url, num_pages=20) {

# first, obtain the html once only.  This way, if we have to test various ways to 
# manipulate it, we can do so on our local copies and not keep hitting Amazon's servers

# Create a directory for this product's reviews, using the product name as the directory name.
directory_name<-str_match(base_url, "amazon.com/(.+)/product-reviews")[2]
dir.create(directory_name, showWarnings = FALSE) # don't bother telling us if the directory already exists

for (i in 1:num_pages) {
  current_url<-paste(base_url,i,sep="")
  filename<-paste(directory_name, "/Page-", i, ".html", sep="")
  download.file(current_url, filename)
  Sys.sleep(4)  # Be a good citizen and don't hammer the server
}

# Now we'll load the first html files and choose just the "reviews" section.  
# Note that classes are prefixed with a period, so we use ".reviews".  
# Within the "reviews" section, we have any number of nodes that we need to parse.   

#We first create a list called "reviews" that's empty, then fill it with reviews from each html file.

reviews<-list()
for (i in 1:length(list.files(directory_name))) {
  amazon_html <- read_html(paste(directory_name, "/Page-", i, ".html", sep=""))
  review_section<-amazon_html %>% html_node(".reviews")
  reviews<-c(reviews, review_section %>% html_nodes(".review"))
}

# Now we create vectors for each element we're going to pull out of each review.

attr(reviews, "class")<-"xml_nodeset"
helpful_votes<-reviews %>% html_nodes(".helpful-votes-count") %>% html_text()
stars<-reviews %>% html_nodes(".a-icon-alt") %>% html_text()
title<-reviews %>% html_nodes(".review-title") %>% html_text()
author<-reviews %>% html_nodes(".author") %>% html_text()
date<-reviews %>% html_nodes(".review-date") %>% html_text()
text<-reviews %>% html_nodes(".review-text") %>% html_text()

# Now we column bind those vectors:
amazon_reviews<-data.frame(cbind(date, title, author, text, stars, helpful_votes), stringsAsFactors = FALSE)
head(amazon_reviews)

# I've still got to do some cleanup:  date, stars, and helpful_votes need to have extraneous text removed.
amazon_reviews$date<-as.Date(amazon_reviews$date, "on %B %d, %Y")
amazon_reviews$stars<-as.numeric(gsub(" out.+", "", amazon_reviews$stars))
amazon_reviews$helpful_votes<-gsub(" of.+", "", amazon_reviews$helpful_votes)
amazon_reviews$helpful_votes<-as.numeric(gsub(",", "", amazon_reviews$helpful_votes))
return(amazon_reviews)
}
```

Now we can use that function to get as many pages of reviews as we want from as many products as we want. Besides the data for the Banana Slicer, we're also interested in reviews on the Bic Cristal For Her pen, the Denon AKDL1 Dedicated Link Cable (Discontinued by Manufacturer), Uranium Ore, and the Samsung UN85S9 Framed 85-Inch 4K Ultra HD 3D Smart LED TV, all of which are characterized by (seemingly 100%) satirical reviews.

Please note that running this script multiple times will result in an error, because of a robot check!

Satire reviews:

```{r}
banana_slicer_reviews<-getReviews("http://www.amazon.com/Hutzler-571-Banana-Slicer/product-reviews/B0047E0EII/?sortBy=byRankDescending&pageNumber=")
bic_reviews<-getReviews("http://www.amazon.com/BIC-Cristal-1-0mm-Black-MSLP16-Blk/product-reviews/B004F9QBE6/?sortBy=bySubmissionDateDescending&pageNumber=")
cable_reviews<-getReviews("http://www.amazon.com/Denon-AKDL1-Dedicated-Discontinued-Manufacturer/product-reviews/B000I1X6PM/?sortBy=bySubmissionDateDescending&pageNumber=")
uranium_reviews<-getReviews("http://www.amazon.com/Images-SI-Inc-Uranium-Ore/product-reviews/B000796XXM/?sortBy=bySubmissionDateDescending&pageNumber=")
tv_reviews<-getReviews("http://www.amazon.com/Samsung-UN85S9-Framed-85-Inch-Ultra/product-reviews/B00CMEN95U/?sortBy=bySubmissionDateDescending&pageNumber=")
```


Serious reviews: 

```{r}
apple_slicer_reviews<-getReviews("http://www.amazon.com/OXO-Grips-Apple-Corer-Divider/product-reviews/B00004OCKT/?sortBy=byRankDescending&pageNumber=")
pen_reviews<-getReviews("http://www.amazon.com/BIC-Velocity-1-6mm-Black-VLGB11-Blk/product-reviews/B004F9QBDC/?sortBy=byRankDescending&pageNumber=")
hdmi_cable_reviews<-getReviews("http://www.amazon.com/Mediabridge-ULTRA-HDMI-Cable-25-Feet/product-reviews/B0031TRZX2/?sortBy=byRankDescending&pageNumber=")
gemstone_reviews<-getReviews("http://www.amazon.com/Madagascar-gemstones-labradorite-septarian-chrysocolla/product-reviews/B003KQZY2K/?sortBy=byRankDescending&pageNumber=")
normal_tv_review<-getReviews("http://www.amazon.com/Samsung-UN48J5200-48-Inch-1080p-Smart/product-reviews/B00WR28U10/?sortBy=byRankDescending&pageNumber=")
```

# Exploring Data

## Basic stats

Here we can do basic stats that don't require text mining: like number of words per review, number of words per title, number of words in all caps, etc.  Again, we'll create a function that will allow us to apply the analysis to any data frame.

__We could also here count non-dictionary words, which might indicate humor / satire ... this could be a way to include a different data source, like a dictionary CSV...  OR we could get a list of high sentiment words, which could also be an indicator.__

Here's a link for offensive terms:  http://www.cs.cmu.edu/~biglou/resources/bad-words.txt
Also I've added a list of positive and negative sentiment words to the repo.

```{r}
offensive<-unlist(read.table("http://www.cs.cmu.edu/~biglou/resources/bad-words.txt"))
# Add spaces on either side to tokenize the offensive.  
# The reason is that we don't want to hit on "ho", let's say, if the word 
# is "holiday".  Only if "ho" is used as a word in and of itself.
offensive<-paste(" ",offensive," ",sep="")
```
__We could add those dictionaries to a free trial of Google SQL...  https://console.cloud.google.com/freetrial?pli=1 ___

```{r}
countWords<-function (df) {
# Count words
df<-mutate(df, num_words_title = stri_count(title,regex="\\S+"))
df<-mutate(df, num_words_review = stri_count(text,regex="\\S+"))
# Count words in all caps that aren't single letter words like I, A, etc.
df<-mutate(df, num_caps_title = stri_count(title,regex="[A-Z]{2,}"))
df<-mutate(df, num_caps_review = stri_count(text,regex="[A-Z]{2,}"))
# Count number of exclamation points
df<-mutate(df, num_excl_title = stri_count(title,regex="!"))
df<-mutate(df, num_excl_review = stri_count(text,regex="!"))
# Count number of question marks
df<-mutate(df, num_quest_title = stri_count(title,regex="\\?"))
df<-mutate(df, num_quest_review = stri_count(text,regex="\\?"))
# Count offensive
df<-mutate(df, num_offensive_title = stri_count(title, regex=paste(offensive, collapse = "|")))
df<-mutate(df, num_offensive_text = stri_count(text, regex=paste(offensive, collapse = "|")))

return(df)
}

banana_slicer_reviews<-countWords(banana_slicer_reviews)
bic_reviews<-countWords(bic_reviews)
cable_reviews<-countWords(cable_reviews)
uranium_reviews<-countWords(uranium_reviews)
tv_reviews<-countWords(tv_reviews)

apple_slicer_reviews<-countWords(apple_slicer_reviews)
pen_reviews<-countWords(gel_pen_reviews)
hdmi_cable_reviews<-countWords(hdmi_cable_reviews)
gemstone_reviews<-countWords(gemstone_reviews)
normal_tv_reviews<-countWords(normal_tv_reviews)
```
If we want to see if the offensive check is legit (maybe we are hitting on words that aren't really offensive in our opinion), we can do something like the following:

```{r}
bad_apples<-apple_slicer_reviews %>% filter (num_offensive_text > 0)
str_match_all(bad_apples$text, paste(offensive, collapse = "|"))
```

In this case, we're hitting on words that aren't really offensive, or only would be contextually.
If we think these terms will be found equally frequently in the satire and serious reviews, we can leave them be for now.  Regardless, we should consider paring down the list and removing some innocuous words like " black ", " knife ", " hole ", etc.  Other terms may still warrant inclusion, like " stupid ".



## Creating Corpora
 
For preprocessing purposes, I will create one corpus for each product.  This will allow me to treat the name of the product as if it were a stopword, ignoring words contained in the product name in the construction of the term document matrix.
(Is this how we want to proceed?  Unsure.)

```{r}
library(tm)
library(SnowballC)
```

```{r}

# I create a function that will take any of my data frames and create 
# a corpus out of it.  I add both review titles and review text to 
# the text.

makeCorpus <-function(corpus_name, df) {
dir.create(corpus_name, showWarnings = FALSE) # don't bother telling us if the directory already exists
file_name<-paste(corpus_name,deparse(substitute(df)), sep="/")
write(paste(df$title,df$text), file_name)
}

makeCorpus("corpus_bic", bic_reviews)
makeCorpus("corpus_banana_slicer", banana_slicer_reviews)
makeCorpus("corpus_cable", cable_reviews)
makeCorpus("corpus_tv", tv_reviews)
makeCorpus("corpus_uranium", uranium_reviews)

# do cleanup here -- stemming, removal of punctuation, etc.
# First we can try a standard bag-of-words, where we remove punctuation, set everything to 
# lower case, and do word stemming. It could be that this does the trick.  If not,
# we will have to do more conservative preprocessing to conserve non-word clues like
# nonstandard punctuation or use of capital letters.

# We create a function to handle this preprocessing.  
# The function takes as input a directory name where the corpus is found an an optional
# string for providing additional stopwords (for example, removing words that are in
# the name of the produc itself).

preprocessCorpus <- function(dir_name, special_stopwords="") {
  corpus<-Corpus(DirSource(dir_name))  
  corpus <- tm_map(corpus, removePunctuation)   
  corpus <- tm_map(corpus, removeNumbers)   
  corpus <- tm_map(corpus, content_transformer(tolower))   
  corpus <- tm_map(corpus, removeWords, stopwords("english")) 
  stopwords_vector<-strsplit(special_stopwords, " ")[[1]]
  corpus <- tm_map(corpus, removeWords, stopwords_vector) # remove special stopwords both before and 
  corpus <- tm_map(corpus, stemDocument) 
  corpus <- tm_map(corpus, removeWords, stopwords_vector) # after stemming.
  return(corpus)
}

banana_slicer_corpus<-preprocessCorpus("corpus_banana_slicer", "hutzler banana slicer")
bic_corpus<-preprocessCorpus("corpus_bic", "bic cristal for her")
cable_corpus<-preprocessCorpus("corpus_cable", "denon akdl dedicated link cable discontinued by manufacturer")
tv_corpus<-preprocessCorpus("corpus_tv", "samsung uns framed inch k ultra hd d smart led tv")
uranium_corpus<-preprocessCorpus("corpus_uranium", "uranium ore")

#not sure the following actually works.
entire_corpus<-c(banana_slicer_corpus, bic_corpus,cable_corpus, tv_corpus, uranium_corpus)

dtm <- DocumentTermMatrix(entire_corpus)   
inspect(dtm)
```

# Modeling Data

For the modeling step, I did some research, and I think the way to go is to use naive Bayes analysis.

It is supposed to be simple code and work well for our type of application.

```{r}
install.packages("klaR", dependencies = TRUE, repos = "http://cran.mirrors.hoobly.com/")
library(klaR)

# First we transform our corpus into a data frame
df <- as.data.frame(dtm)


# set up a training sample (70% of the data frame)
df_train <- sample(1:nrow(df), ceiling(nrow(df)* 0.7), replace=FALSE)

# apply NB classifier
nb.res <- NaiveBayes(satire ~ ., data=df[df_train,])

# show the results
opar <- par(mfrow=c(2,4))
plot(nb.res)
par(opar)

# predict on holdout units
nb.pred <- predict(nb.res, df[-df_train,])

# raw accuracy
confusion.mat <- table(nb.pred$class, df[-df_train,"satire"])
sum(diag(confusion.mat))/sum(confusion.mat)

# Or we could use code like this: (lets see which way works)


sub = sample(nrow(df), floor(nrow(df) * 0.8))
train = df[sub,]
test = df[-sub,]

xTrain = train[,-classificationcolnumber]
yTrain = train$satire

xTest = test[,-classificationcolnumber]
yTest = test$satire

model = train(xTrain,yTrain,'nb',trControl=trainControl(method='cv',number=10))

prop.table(table(predict(model$finalModel,xTest)$class,yTest))


```



# Interpreting Data

